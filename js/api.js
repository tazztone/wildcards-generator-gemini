// @ts-nocheck
import { Config } from './config.js';

export const Api = {
    activeController: null,

    async _makeRequest(globalPrompt, userPrompt, generationConfig) {
        if (this.activeController) this.activeController.abort();
        this.activeController = new AbortController();

        try {
            const { url, payload, headers } = this._prepareRequest(globalPrompt, userPrompt, generationConfig);
            const makeRequest = async (currentPayload) => {
                const res = await fetch(url, {
                    method: 'POST',
                    headers,
                    body: JSON.stringify(currentPayload),
                    signal: AbortSignal.any([this.activeController.signal, AbortSignal.timeout(30000)])
                });
                if (!res.ok) {
                    const text = await res.text();
                    return { ok: false, status: res.status, text };
                }
                return { ok: true, json: await res.json() };
            };

            let reqResult = await makeRequest(payload);

            // Retry for LMStudio/OpenAI strictness
            if (!reqResult.ok && reqResult.status === 400 && payload.response_format) {
                const errText = reqResult.text;
                if (errText.includes("must be 'json_schema' or 'text'")) {
                    console.warn("LMStudio strict JSON mode detected, retrying with json_schema...");
                    const schema = this._constructJsonSchema(generationConfig);
                    payload.response_format = { type: "json_schema", json_schema: schema };
                    reqResult = await makeRequest(payload);
                } else if (errText.includes('JSON mode') || errText.includes('not supported') || errText.includes('INVALID_ARGUMENT')) {
                    console.warn("JSON mode failed, retrying without response_format...");
                    delete payload.response_format;
                    reqResult = await makeRequest(payload);
                }
            }

            if (!reqResult.ok) {
                throw new Error(`API request failed: ${reqResult.status} - ${reqResult.text}`);
            }

            const result = reqResult.json;
            return { result, request: { url, headers, payload } };
        } catch (error) {
            if (error.name === 'AbortError') throw new Error("Request timed out or was aborted.");
            console.error("Error calling LLM API:", error);
            throw error;
        } finally {
            this.activeController = null;
        }
    },

    /**
     * Make a streaming request to the LLM API with progress callbacks.
     * Uses SSE (Server-Sent Events) format to receive streamed responses.
     */
    async _makeStreamingRequest(globalPrompt, userPrompt, generationConfig, onProgress) {
        if (this.activeController) this.activeController.abort();
        this.activeController = new AbortController();
        const startTime = Date.now();

        try {
            const { url, payload, headers } = this._prepareRequest(globalPrompt, userPrompt, generationConfig);
            payload.stream = true; // Enable streaming

            const makeStreamingRequest = async (currentPayload) => {
                const res = await fetch(url, {
                    method: 'POST',
                    headers,
                    body: JSON.stringify(currentPayload),
                    signal: AbortSignal.any([this.activeController.signal, AbortSignal.timeout(60000)])
                });
                if (!res.ok) {
                    const text = await res.text();
                    return { ok: false, status: res.status, text };
                }
                return { ok: true, body: res.body };
            };

            let reqResult = await makeStreamingRequest(payload);

            // Retry for LMStudio/OpenAI strictness (Streaming)
            if (!reqResult.ok && reqResult.status === 400 && payload.response_format) {
                const errText = reqResult.text;
                if (errText.includes("must be 'json_schema' or 'text'")) {
                    console.warn("LMStudio strict JSON mode detected (streaming), retrying with json_schema...");
                    const schema = this._constructJsonSchema(generationConfig);
                    payload.response_format = { type: "json_schema", json_schema: schema };
                    reqResult = await makeStreamingRequest(payload);
                } else if (errText.includes('JSON mode') || errText.includes('not supported') || errText.includes('INVALID_ARGUMENT')) {
                    console.warn("JSON mode failed, retrying without response_format...");
                    delete payload.response_format;
                    reqResult = await makeStreamingRequest(payload);
                }
            }

            if (!reqResult.ok) {
                throw new Error(`API request failed: ${reqResult.status} - ${reqResult.text}`);
            }

            // Parse SSE stream
            const reader = reqResult.body.getReader();
            const decoder = new TextDecoder();
            let accumulatedText = '';
            let buffer = '';

            while (true) {
                const { done, value } = await reader.read();
                if (done) break;

                buffer += decoder.decode(value, { stream: true });
                const lines = buffer.split('\n');
                buffer = lines.pop(); // Keep incomplete line in buffer

                for (const line of lines) {
                    if (line.startsWith('data: ')) {
                        const data = line.slice(6).trim();
                        if (data === '[DONE]') continue;

                        try {
                            const parsed = JSON.parse(data);
                            // Handle both OpenRouter/OpenAI and Gemini formats
                            const content = parsed.choices?.[0]?.delta?.content ||
                                parsed.candidates?.[0]?.content?.parts?.[0]?.text || '';
                            if (content) {
                                accumulatedText += content;
                                if (onProgress) {
                                    onProgress({
                                        text: accumulatedText,
                                        elapsed: Date.now() - startTime
                                    });
                                }
                            }
                        } catch (e) {
                            // Ignore parse errors for malformed chunks
                        }
                    }
                }
            }

            return {
                result: { text: accumulatedText },
                elapsed: Date.now() - startTime,
                request: { url, headers, payload }
            };
        } catch (error) {
            if (error.name === 'AbortError') throw new Error("Request timed out or was aborted.");
            console.error("Error calling LLM API (streaming):", error);
            throw error;
        } finally {
            this.activeController = null;
        }
    },

    async generateWildcards(globalPrompt, categoryPath, existingWords, customInstructions, systemPrompt) {
        const readablePath = categoryPath.replace(/\//g, ' > ').replace(/_/g, ' ');
        const userPrompt = `Category Path: '${readablePath}'\nExisting Wildcards: ${existingWords.slice(0, 50).join(', ')}\nCustom Instructions: "${customInstructions.trim()}"`;
        const generationConfig = { responseMimeType: "application/json", responseSchema: { type: "ARRAY", items: { type: "STRING" } } };

        const { result } = await this._makeRequest(globalPrompt, userPrompt, generationConfig);
        return this._parseResponse(result);
    },

    async suggestItems(parentPath, structure, suggestItemPrompt) {
        const readablePath = parentPath ? parentPath.replace(/\//g, ' > ').replace(/_/g, ' ') : 'Top-Level';
        const globalPrompt = suggestItemPrompt.replace('{parentPath}', readablePath);
        const userPrompt = `For context, here are the existing sibling items at the same level:\n${JSON.stringify(structure, null, 2)}\n\nPlease provide new suggestions for the '${readablePath}' category.`;
        const generationConfig = {
            responseMimeType: "application/json",
            responseSchema: {
                type: "ARRAY",
                items: {
                    type: "OBJECT",
                    properties: {
                        name: {
                            type: "STRING",
                            description: "A unique, descriptive name for a new sub-category. MUST use underscores_between_words. MUST NOT be a generic placeholder. MUST NOT contain the parent category's name."
                        },
                        instruction: {
                            type: "STRING",
                            description: "A brief, helpful description of the item's purpose."
                        }
                    },
                    required: ["name", "instruction"]
                }
            }
        };

        const { result, request } = await this._makeRequest(globalPrompt, userPrompt, generationConfig);
        return { suggestions: this._parseResponse(result), request };
    },

    /**
     * Generate prompt templates by combining wildcard category paths.
     * Uses path mapping to optimize token usage.
     * @param {Object<string, string>} pathMap - Mapping of short codes to full paths
     * @param {string} instructions - Custom instructions for template style
     * @param {string} templatePrompt - The system prompt for template generation
     * @returns {Promise<string[]>} Array of generated template strings with full paths
     */
    async generateTemplates(pathMap, instructions, templatePrompt) {
        // Build readable path context for LLM
        const pathContext = Object.entries(pathMap)
            .map(([code, path]) => `${code} = "${path.replace(/\//g, ' > ').replace(/_/g, ' ')}"`)
            .join('\n');

        const userPrompt = `PATH MAP:\n${pathContext}\n\nINSTRUCTIONS: ${instructions}`;
        const generationConfig = {
            responseMimeType: "application/json",
            responseSchema: { type: "ARRAY", items: { type: "STRING" } }
        };

        const { result } = await this._makeRequest(templatePrompt, userPrompt, generationConfig);
        let templates = this._parseResponse(result);

        // Validation BEFORE expansion (codes are still A, B, AA format)
        const validCodes = new Set(Object.keys(pathMap));
        const seen = new Set();

        templates = templates.filter(t => {
            t = String(t).trim();
            if (!t || seen.has(t)) return false;
            seen.add(t);

            // Find all __CODE__ placeholders (uppercase letters only)
            const placeholders = t.match(/__([A-Z]+)__/g) || [];

            // Require at least 2 different codes per template
            if (placeholders.length < 2) return false;

            // All codes must be in our valid set
            const allValid = placeholders.every(p => {
                const code = p.replace(/__/g, '');
                return validCodes.has(code);
            });
            return allValid;
        });

        // NOW expand valid templates to full paths
        return templates.map(t => {
            let expanded = t;
            for (const [code, path] of Object.entries(pathMap)) {
                // Use exact stored path (no case normalization)
                expanded = expanded.replace(new RegExp(`__${code}__`, 'g'), `__${path}__`);
            }
            return expanded;
        });
    },

    /**
     * Use AI to pick the best category for each duplicate wildcard.
     * Processes duplicates in configurable batches with optional parallelism and cooldown.
     * @param {Array} duplicates - Array of {normalized, locations, count} from findDuplicates
     * @param {{batchSize?: number, parallelRequests?: number, cooldownMs?: number}} options
     * @param {function({processed: number, total: number}): void} [onProgress] - Progress callback
     * @returns {Promise<Map<string, string>>} Map of normalized wildcard â†’ path to keep
     */
    async pickBestCategoryForDuplicates(duplicates, options = {}, onProgress = null) {
        const { batchSize = 10, parallelRequests = 1, cooldownMs = 0 } = options;
        const decisions = new Map();

        if (!duplicates || duplicates.length === 0) return decisions;

        // Split into batches
        const batches = [];
        for (let i = 0; i < duplicates.length; i += batchSize) {
            batches.push(duplicates.slice(i, i + batchSize));
        }

        // Use configurable prompt (allows user customization in future)
        const systemPrompt = Config.DEFAULT_DEDUPLICATE_PROMPT;

        const generationConfig = {
            responseMimeType: "application/json",
            responseSchema: {
                type: "ARRAY",
                items: {
                    type: "OBJECT",
                    properties: {
                        wildcard: { type: "STRING", description: "The normalized duplicate wildcard" },
                        keep_path: { type: "STRING", description: "Full path to the category that should keep this wildcard" }
                    },
                    required: ["wildcard", "keep_path"]
                }
            }
        };

        let processed = 0;

        // Process batches with parallelism
        for (let i = 0; i < batches.length; i += parallelRequests) {
            const parallelBatches = batches.slice(i, i + parallelRequests);

            const batchPromises = parallelBatches.map(async (batch) => {
                // Build user prompt for this batch
                const batchPrompt = batch.map(d => {
                    const pathOptions = d.locations.map(l =>
                        `  - ${l.path.replace(/\//g, ' > ').replace(/_/g, ' ')}`
                    ).join('\n');
                    return `Wildcard: "${d.normalized}"\nFound in:\n${pathOptions}`;
                }).join('\n\n');

                const userPrompt = `Please analyze these ${batch.length} duplicate wildcards and pick the best category for each:\n\n${batchPrompt}`;

                try {
                    const { result } = await this._makeRequest(systemPrompt, userPrompt, generationConfig);
                    const parsed = this._parseResponse(result);

                    // Store decisions
                    if (Array.isArray(parsed)) {
                        parsed.forEach(item => {
                            if (item.wildcard && item.keep_path) {
                                decisions.set(item.wildcard.toLowerCase().trim(), item.keep_path);
                            }
                        });
                    }

                    processed += batch.length;
                    if (onProgress) {
                        onProgress({ processed, total: duplicates.length });
                    }
                } catch (error) {
                    console.error('AI batch processing failed:', error);
                    // On error, don't add decisions for this batch (will fallback in cleanDuplicates)
                    processed += batch.length;
                    if (onProgress) {
                        onProgress({ processed, total: duplicates.length });
                    }
                }
            });

            await Promise.all(batchPromises);

            // Apply cooldown between batch groups (not after last)
            if (cooldownMs > 0 && i + parallelRequests < batches.length) {
                await new Promise(resolve => setTimeout(resolve, cooldownMs));
            }
        }

        return decisions;
    },

    async testConnection(provider, uiCallback, explicitKey = null) {
        if (uiCallback) uiCallback(`Testing connection to ${provider}...`, 'info');

        try {
            let url, requestOptions = { method: 'GET', headers: {} };

            // Helper to get key: use explicit argument first, then DOM
            const getKey = (id) => explicitKey || document.getElementById(id)?.value?.trim() || '';
            const getVal = (id) => document.getElementById(id)?.value?.trim() || '';

            if (provider === 'gemini') {
                const apiKey = getKey('gemini-api-key');
                if (!apiKey) throw new Error("Gemini API key not provided.");
                url = `https://generativelanguage.googleapis.com/v1beta/models?key=${apiKey}`;
            } else if (provider === 'openrouter') {
                const apiKey = getKey('openrouter-api-key');
                if (!apiKey) throw new Error("OpenRouter API key not provided.");

                // 1. Verify Key first using /auth/key endpoint
                const authUrl = 'https://openrouter.ai/api/v1/auth/key';
                const authResponse = await fetch(authUrl, {
                    method: 'GET',
                    headers: { 'Authorization': `Bearer ${apiKey}` }
                });

                if (authResponse.status === 401) {
                    throw new Error("Invalid OpenRouter API Key.");
                } else if (!authResponse.ok) {
                    // Some other error, but let's try reading text
                    const text = await authResponse.text();
                    throw new Error(`OpenRouter Auth Check Failed: ${authResponse.status} - ${text}`);
                }

                // 2. Fetch Models
                url = `https://openrouter.ai/api/v1/models`;
                requestOptions.headers['Authorization'] = `Bearer ${apiKey}`;

            } else if (provider === 'custom') {
                const customUrl = getVal('custom-api-url');
                if (!customUrl) throw new Error("Custom API URL is not provided.");
                url = `${customUrl.replace(/\/$/, '')}/models`;
                const apiKey = getKey('custom-api-key');
                if (apiKey) {
                    requestOptions.headers['Authorization'] = `Bearer ${apiKey}`;
                }
            }

            if (!url) throw new Error("Could not determine URL for testing.");

            const response = await fetch(url, requestOptions);
            if (!response.ok) {
                const errorText = await response.text().catch(() => 'Could not retrieve error details.');
                throw new Error(`Request failed: ${response.status} ${response.statusText}. Response: ${errorText}`);
            }
            const data = await response.json();

            let successMessage = '';
            let models = [];

            if (provider === 'gemini') {
                if (!data.models) throw new Error('Invalid response from Gemini API.');
                successMessage = `Gemini connection successful! Found ${data.models.length} models.`;
                models = data.models;
            } else if (provider === 'openrouter') {
                const list = Array.isArray(data) ? data : (data.data || []);
                if (!list.length && !Array.isArray(list)) throw new Error('Invalid response from OpenRouter API.');

                successMessage = `OpenRouter key verified! Found ${list.length} models.`;
                models = list;
            } else if (provider === 'custom') {
                const list = Array.isArray(data) ? data : (data.data || []);
                successMessage = `Custom API connection successful! Found ${list.length} models.`;
                models = list;
            }

            if (uiCallback) uiCallback(successMessage, 'success');
            return models; // Return models so UI can populate lists

        } catch (error) {
            console.error("Connection Test Error:", error);
            let message = `Connection failed: ${error.message}`;
            if (error instanceof TypeError && error.message === 'Failed to fetch') {
                message += '\n\nThis is likely a Cross-Origin Resource Sharing (CORS) issue.';
            }
            if (uiCallback) uiCallback(message, 'error'); // Use 'error' type for notification
            throw error;
        }
    },

    _prepareRequest(globalPrompt, userPrompt, generationConfig = {}) {
        const endpoint = document.getElementById('api-endpoint').value; // Still reading DOM for active endpoint
        let apiKey, url, payload, headers = { 'Content-Type': 'application/json' };

        const getKey = (id) => document.getElementById(id)?.value?.trim() || '';
        const getVal = (id) => document.getElementById(id)?.value?.trim() || '';

        // Common Parameters from Config - only use if not default
        const temp = Config.MODEL_TEMPERATURE;
        const maxTokens = Config.MODEL_MAX_TOKENS;
        const topP = Config.MODEL_TOP_P;
        const topK = Config.MODEL_TOP_K ?? 0;

        if (endpoint === 'gemini') {
            apiKey = getKey('gemini-api-key');
            const model = getVal('gemini-model-name') || 'gemini-1.5-flash';
            if (!apiKey) throw new Error("Gemini API key is not provided.");
            url = `https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${apiKey}`;

            const geminiGenConfig = {
                ...generationConfig // Allow overrides
            };
            // Only include parameters if they differ from defaults
            if (temp !== undefined && temp !== 0.7) geminiGenConfig.temperature = temp;
            if (maxTokens && maxTokens !== 1000) geminiGenConfig.maxOutputTokens = maxTokens;
            if (topP !== undefined && topP !== 1.0) geminiGenConfig.topP = topP;
            if (topK > 0) geminiGenConfig.topK = topK;

            payload = {
                contents: [
                    { role: "user", parts: [{ text: globalPrompt }] },
                    { role: "model", parts: [{ text: "Understood." }] },
                    { role: "user", parts: [{ text: userPrompt }] }
                ],
                generationConfig: geminiGenConfig
            };
        } else if (endpoint === 'openrouter' || endpoint === 'custom') {
            const isCustom = endpoint === 'custom';
            apiKey = getKey(isCustom ? 'custom-api-key' : 'openrouter-api-key');
            const model = getVal(isCustom ? 'custom-model-name' : 'openrouter-model-name') || (isCustom ? "" : ":free");

            if (!isCustom && !apiKey) throw new Error("OpenRouter API key is not provided.");

            if (isCustom) {
                const customUrl = getVal('custom-api-url');
                if (!customUrl) throw new Error("Custom API URL is not provided.");
                url = `${customUrl.replace(/\/$/, '')}/chat/completions`;
            } else {
                url = `https://openrouter.ai/api/v1/chat/completions`;
            }

            if (apiKey) headers['Authorization'] = `Bearer ${apiKey}`;

            payload = {
                model,
                messages: [
                    { role: "user", content: `${globalPrompt}\n\n${userPrompt}` }
                ]
            };

            // Only include parameters if they differ from defaults
            if (temp !== undefined && temp !== 0.7) payload.temperature = temp;
            if (maxTokens && maxTokens !== 1000) payload.max_tokens = maxTokens;
            if (topP !== undefined && topP !== 1.0) payload.top_p = topP;

            // Add extended parameters if not default
            if (topK > 0) payload.top_k = topK;
            if (Config.MODEL_FREQUENCY_PENALTY !== 0) payload.frequency_penalty = Config.MODEL_FREQUENCY_PENALTY;
            if (Config.MODEL_PRESENCE_PENALTY !== 0) payload.presence_penalty = Config.MODEL_PRESENCE_PENALTY;
            if (Config.MODEL_REPETITION_PENALTY !== 1) payload.repetition_penalty = Config.MODEL_REPETITION_PENALTY;
            if (Config.MODEL_MIN_P > 0) payload.min_p = Config.MODEL_MIN_P;
            if (Config.MODEL_TOP_A > 0) payload.top_a = Config.MODEL_TOP_A;
            if (Config.MODEL_SEED > 0) payload.seed = Config.MODEL_SEED;



            // Reasoning Parameters
            const reasoning = {};
            if (Config.MODEL_REASONING_EFFORT && Config.MODEL_REASONING_EFFORT !== 'default') {
                reasoning.effort = Config.MODEL_REASONING_EFFORT;
            }
            if (Config.MODEL_REASONING_MAX_TOKENS > 0) {
                reasoning.max_tokens = Config.MODEL_REASONING_MAX_TOKENS;
            }
            // Only add reasoning object if it has properties
            if (Object.keys(reasoning).length > 0) {
                payload.reasoning = reasoning;
            }

            payload.response_format = { type: "json_object" };
        } else {
            throw new Error("Invalid API endpoint.");
        }
        return { url, payload, headers };
    },

    _parseResponse(result) {
        const endpoint = document.getElementById('api-endpoint').value;
        try {
            if (endpoint === 'gemini') return JSON.parse(result.candidates[0].content.parts[0].text);
            if (endpoint === 'openrouter' || endpoint === 'custom') {
                let contentStr = result.choices[0].message.content.trim();
                const match = /```(?:json)?\s*([\s\S]*?)\s*```/.exec(contentStr);
                if (match) contentStr = match[1];
                const content = JSON.parse(contentStr);
                return Array.isArray(content) ? content : content.wildcards || content.categories || content.items || [];
            }
            return [];
        } catch (e) {
            console.error("Failed to parse AI response:", result, e);
            throw new Error(`The AI returned a malformed response. Error: ${e.message}`);
        }
    },

    /**
     * Test the currently selected model with a realistic wildcard generation request.
     * Returns stats including response time, JSON support, and raw response for debugging.
     */
    async testModel(provider, apiKey, modelName, uiCallback) {
        const startTime = performance.now();

        // Load real data from initial-data.yaml to make the test realistic
        let testCategoryPath = 'CREATURES_and_BEINGS/Mythical_Fantasy';
        let testExistingItems = ['dragon', 'griffin', 'unicorn', 'fairy', 'goblin', 'troll'];
        let testInstruction = 'Legendary and fantasy creatures';

        try {
            const yamlResponse = await fetch('data/initial-data.yaml');
            if (yamlResponse.ok) {
                const yamlText = await yamlResponse.text();
                const YAML = await import('https://cdn.jsdelivr.net/npm/yaml@2.3.4/browser/index.js');
                const data = YAML.parse(yamlText);

                // Try to get real data from CREATURES_and_BEINGS > Mythical_Fantasy
                if (data?.CREATURES_and_BEINGS?.Mythical_Fantasy) {
                    testCategoryPath = 'CREATURES_and_BEINGS/Mythical_Fantasy';
                    testExistingItems = data.CREATURES_and_BEINGS.Mythical_Fantasy || [];
                    testInstruction = 'Legendary and fantasy creatures';
                }
            }
        } catch (e) {
            console.warn('Failed to load initial-data.yaml for test, using fallback data', e);
        }

        // Use the actual system prompt from the app's config
        const systemPrompt = Config.DEFAULT_SYSTEM_PROMPT ||
            "You are a creative assistant for generating wildcards for AI image prompts. You will be given a category path, a list of existing wildcards, and optional custom instructions. Your task is to generate 20 more diverse and creative wildcards that fit the category. Do not repeat any from the existing list. Follow all custom instructions. Return ONLY the new wildcards as a JSON array of strings.";

        // Format the test exactly like the app does for wildcard generation
        const readablePath = testCategoryPath.replace(/\//g, ' > ').replace(/_/g, ' ');
        const userPrompt = `Category Path: '${readablePath}'\nExisting Wildcards: ${testExistingItems.slice(0, 20).join(', ')}\nCustom Instructions: "${testInstruction}"`;

        try {
            let url, headers, payload;

            // Only include parameters if they differ from defaults
            const temp = Config.MODEL_TEMPERATURE;
            const maxTokens = Config.MODEL_MAX_TOKENS;
            const topP = Config.MODEL_TOP_P;
            const topK = Config.MODEL_TOP_K ?? 0;

            if (provider === 'gemini') {
                url = `https://generativelanguage.googleapis.com/v1beta/models/${modelName}:generateContent?key=${apiKey}`;
                headers = { 'Content-Type': 'application/json' };

                const geminiGenConfig = {
                    responseMimeType: 'application/json',
                    responseSchema: { type: 'ARRAY', items: { type: 'STRING' } }
                };
                // Only include if different from defaults
                if (temp !== undefined && temp !== 0.7) geminiGenConfig.temperature = temp;
                if (maxTokens && maxTokens !== 1000) geminiGenConfig.maxOutputTokens = maxTokens;
                if (topP !== undefined && topP !== 1.0) geminiGenConfig.topP = topP;
                if (topK > 0) geminiGenConfig.topK = topK;

                payload = {
                    contents: [
                        { role: "user", parts: [{ text: systemPrompt }] },
                        { role: "model", parts: [{ text: "Understood." }] },
                        { role: "user", parts: [{ text: userPrompt }] }
                    ],
                    generationConfig: geminiGenConfig
                };
            } else if (provider === 'openrouter' || provider === 'custom') {
                const isCustom = provider === 'custom';

                if (isCustom) {
                    const baseUrl = document.getElementById('custom-api-url')?.value || Config?.API_URL_CUSTOM || '';
                    url = baseUrl.replace(/\/$/, '') + '/chat/completions';
                } else {
                    url = 'https://openrouter.ai/api/v1/chat/completions';
                }

                headers = {
                    'Content-Type': 'application/json',
                    ...(apiKey && { 'Authorization': `Bearer ${apiKey}` }),
                    ...(provider === 'openrouter' && { 'HTTP-Referer': window.location.origin })
                };

                payload = {
                    model: modelName,
                    messages: [
                        { role: 'user', content: `${systemPrompt}\n\n${userPrompt}` }
                    ],
                    response_format: { type: 'json_object' }
                };

                // Only include if different from defaults
                if (temp !== undefined && temp !== 0.7) payload.temperature = temp;
                if (maxTokens && maxTokens !== 1000) payload.max_tokens = maxTokens;
                if (topP !== undefined && topP !== 1.0) payload.top_p = topP;

                // Add extended parameters if not default
                if (topK > 0) payload.top_k = topK;
                if (Config.MODEL_FREQUENCY_PENALTY !== 0) payload.frequency_penalty = Config.MODEL_FREQUENCY_PENALTY;
                if (Config.MODEL_PRESENCE_PENALTY !== 0) payload.presence_penalty = Config.MODEL_PRESENCE_PENALTY;
                if (Config.MODEL_REPETITION_PENALTY !== 1) payload.repetition_penalty = Config.MODEL_REPETITION_PENALTY;
                if (Config.MODEL_MIN_P > 0) payload.min_p = Config.MODEL_MIN_P;
                if (Config.MODEL_TOP_A > 0) payload.top_a = Config.MODEL_TOP_A;
                if (Config.MODEL_SEED > 0) payload.seed = Config.MODEL_SEED;

                // Reasoning Parameters
                const reasoning = {};
                if (Config.MODEL_REASONING_EFFORT && Config.MODEL_REASONING_EFFORT !== 'default') {
                    reasoning.effort = Config.MODEL_REASONING_EFFORT;
                }
                if (Config.MODEL_REASONING_MAX_TOKENS > 0) {
                    reasoning.max_tokens = Config.MODEL_REASONING_MAX_TOKENS;
                }
                if (Object.keys(reasoning).length > 0) {
                    payload.reasoning = reasoning;
                }
            }

            // Fallback logic for models that don't support json_object
            const makeRequest = async (currentPayload) => {
                const res = await fetch(url, { method: 'POST', headers, body: JSON.stringify(currentPayload) });
                if (!res.ok) {
                    const text = await res.text();
                    return { ok: false, status: res.status, text };
                }
                return { ok: true, json: await res.json() };
            };

            let reqResult = await makeRequest(payload);
            const duration = Math.round(performance.now() - startTime);

            // Retry if 400 error regarding JSON mode (OpenAI generic) or LMStudio specific strictness
            if (!reqResult.ok && reqResult.status === 400 &&
                payload.response_format) {

                const errText = reqResult.text;
                // LMStudio specific error: "must be 'json_schema' or 'text'"
                if (errText.includes("must be 'json_schema' or 'text'")) {
                    console.warn("LMStudio strict JSON mode detected, retrying with json_schema...");

                    // Convert standard config to JSON Schema
                    // For testModel, we fundamentally want an array of strings
                    const schema = this._constructJsonSchema({
                        responseSchema: { type: "ARRAY", items: { type: "STRING" } }
                    });

                    payload.response_format = {
                        type: "json_schema",
                        json_schema: schema
                    };
                    reqResult = await makeRequest(payload);

                } else if (errText.includes('JSON mode') || errText.includes('not supported') || errText.includes('INVALID_ARGUMENT')) {
                    // Fallback for others: just remove response_format
                    console.warn("JSON mode failed, retrying without response_format...");
                    delete payload.response_format;
                    reqResult = await makeRequest(payload);
                }
            }

            if (!reqResult.ok) {
                let params = `HTTP ${reqResult.status}`;
                try {
                    const errJson = JSON.parse(reqResult.text);
                    if (errJson.error && errJson.error.message) {
                        params += `: ${errJson.error.message}`;
                        if (errJson.error.metadata && errJson.error.metadata.raw) {
                            params += `\nDetails: ${errJson.error.metadata.raw}`;
                        }
                    } else if (errJson.message) {
                        params += `: ${errJson.message}`;
                    } else {
                        params += `: ${reqResult.text}`;
                    }
                } catch (e) {
                    params += `: ${reqResult.text}`;
                }
                throw new Error(params);
            }

            const result = reqResult.json;

            // Extract the response content
            const message = result.choices?.[0]?.message;
            let rawContent = message?.content || result.candidates?.[0]?.content?.parts?.[0]?.text || '';

            // Check for reasoning content (OpenRouter/thinking models)
            if (message?.reasoning) {
                rawContent = `[Reasoning]\n${message.reasoning}\n\n[Content]\n${rawContent}`;
            } else if (message?.reasoning_content) {
                rawContent = `[Reasoning]\n${message.reasoning_content}\n\n[Content]\n${rawContent}`;
            }

            // Check if it's valid JSON array or object
            let parsedContent = null;
            let supportsJson = false;
            try {
                let contentToParse = rawContent;
                // If we combined reasoning and content, we only want to parse the actual content part for JSON
                if (contentToParse.includes('[Content]\n')) {
                    contentToParse = contentToParse.split('[Content]\n')[1] || '';
                }
                contentToParse = contentToParse.trim();

                // Handle markdown code blocks
                const match = /```(?:json)?\s*([\s\S]*?)\s*```/.exec(contentToParse);
                if (match) {
                    contentToParse = match[1];
                }

                parsedContent = JSON.parse(contentToParse);

                if (Array.isArray(parsedContent)) {
                    supportsJson = parsedContent.length > 0;
                } else if (typeof parsedContent === 'object' && parsedContent !== null) {
                    // Check for common wrapper keys or if it's just a valid object (which implies JSON support)
                    // If we used json_schema with a wrapper key (like "items"), we need to extract it
                    if (parsedContent.items && Array.isArray(parsedContent.items)) {
                        parsedContent = parsedContent.items; // Unwrap for count/display
                        supportsJson = true;
                    } else {
                        supportsJson = true;
                        // Try to find array for count
                        const values = Object.values(parsedContent);
                        const foundArray = values.find(v => Array.isArray(v));
                        if (foundArray) {
                            parsedContent = foundArray; // Use this for count
                        }
                    }
                }
            } catch (e) {
                // Fallback: loose check
                supportsJson = rawContent.includes('[') && rawContent.includes(']');
            }

            const stats = {
                responseTime: duration,
                modelName: modelName,
                supportsJson: supportsJson,
                provider: provider,
                rawResponse: rawContent,
                parsedContent: parsedContent, // Pass the full parsed object
                parsedCount: Array.isArray(parsedContent) ? parsedContent.length : 0,
                usage: result.usage || null,
                request: { url, headers, payload } // Return request details
            };

            if (uiCallback) {
                uiCallback({ success: true, stats });
            }

            return stats;
        } catch (error) {
            const duration = Math.round(performance.now() - startTime);
            if (uiCallback) {
                uiCallback({ success: false, error: error.message, responseTime: duration });
            }
            throw error;
        }
    },

    /**
     * Helper to construct a JSON schema object compatible with OpenAI/LMStudio
     * from our internal generationConfig format.
     */
    _constructJsonSchema(generationConfig) {
        // Default schema if none provided: wrapper object with 'items' array of strings
        const defaultSchema = {
            type: "object",
            properties: {
                items: {
                    type: "array",
                    items: { type: "string" }
                }
            },
            required: ["items"]
        };

        if (!generationConfig || !generationConfig.responseSchema) {
            return {
                name: "wildcard_response",
                strict: true,
                schema: defaultSchema
            };
        }

        const internalSchema = generationConfig.responseSchema;
        let finalSchema = {};

        // Convert Google-style schema to OpenAI JSON Schema
        // Case 1: Array of Strings (most common for us)
        if (internalSchema.type === 'ARRAY' && internalSchema.items && internalSchema.items.type === 'STRING') {
            finalSchema = defaultSchema;
        }
        // Case 2: Array of Objects (used for suggestions)
        else if (internalSchema.type === 'ARRAY' && internalSchema.items && internalSchema.items.type === 'OBJECT') {
            // Need to wrap array in an object for JSON Schema root
            const itemProps = {};
            const requiredProps = [];

            if (internalSchema.items.properties) {
                for (const [key, prop] of Object.entries(internalSchema.items.properties)) {
                    itemProps[key] = {
                        type: prop.type.toLowerCase(),
                        description: prop.description
                    };
                }
            }
            if (internalSchema.items.required) {
                requiredProps.push(...internalSchema.items.required);
            }

            finalSchema = {
                type: "object",
                properties: {
                    items: {
                        type: "array",
                        items: {
                            type: "object",
                            properties: itemProps,
                            required: requiredProps,
                            additionalProperties: false
                        }
                    }
                },
                required: ["items"],
                additionalProperties: false
            };
        }
        else {
            // Fallback to default
            finalSchema = defaultSchema;
        }

        return {
            name: "wildcard_response",
            strict: true, // LMStudio requires strict: true (as string "true" or boolean? Docs say "true", OpenAI says boolean true. Let's try boolean.)
            schema: finalSchema
        };
    },

    // ========== BENCHMARK TEST METHODS ==========

    /**
     * Test the Suggestions feature (suggestItems).
     * Uses real data and the configured suggestion prompt.
     */
    async testSuggestions(provider, apiKey, modelName) {
        const startTime = performance.now();

        // Use the configured suggestion prompt
        const basePrompt = Config.DEFAULT_SUGGEST_ITEM_PROMPT ||
            "You are an expert creative assistant. Your task is to suggest new sub-category names.";
        const parentPath = 'CREATURES_and_BEINGS';
        const globalPrompt = basePrompt.replace('{parentPath}', parentPath);

        // Mock sibling structure for context
        const siblingStructure = {
            'Mythical_Fantasy': { instruction: 'Dragons, unicorns, and fantasy creatures' },
            'Animals': { instruction: 'Real-world animals' }
        };

        const userPrompt = `For context, here are the existing sibling items at the same level:\n${JSON.stringify(siblingStructure, null, 2)}\n\nPlease provide new suggestions for the '${parentPath}' category.`;

        const generationConfig = {
            responseMimeType: "application/json",
            responseSchema: {
                type: "ARRAY",
                items: {
                    type: "OBJECT",
                    properties: {
                        name: { type: "STRING" },
                        instruction: { type: "STRING" }
                    },
                    required: ["name", "instruction"]
                }
            }
        };

        try {
            const { result, request } = await this._makeTestRequest(provider, apiKey, modelName, globalPrompt, userPrompt, generationConfig);
            const duration = Math.round(performance.now() - startTime);

            const parsed = this._parseTestResponse(provider, result);
            const isValidArray = Array.isArray(parsed) && parsed.length > 0;
            const hasCorrectShape = isValidArray && parsed[0]?.name && parsed[0]?.instruction;

            return {
                success: true,
                stats: {
                    responseTime: duration,
                    supportsJson: isValidArray,
                    validSchema: hasCorrectShape,
                    parsedCount: isValidArray ? parsed.length : 0,
                    parsedContent: parsed,
                    rawResponse: this._extractRawContent(result, provider),
                    request
                }
            };
        } catch (error) {
            return {
                success: false,
                error: error.message,
                stats: { responseTime: Math.round(performance.now() - startTime) }
            };
        }
    },

    /**
     * Test the Template generation feature.
     * Uses real data and the configured template prompt.
     */
    async testTemplates(provider, apiKey, modelName) {
        const startTime = performance.now();

        const templatePrompt = Config.DEFAULT_TEMPLATE_PROMPT ||
            "You are a Template Architect. Create prompt templates using __CODE__ syntax.";

        // Mock path map like the real feature uses
        const pathMap = {
            'A': 'CREATURES_and_BEINGS/Mythical_Fantasy',
            'B': 'ACTIONS/Movement',
            'C': 'PLACES/Natural_Environments'
        };

        const pathContext = Object.entries(pathMap)
            .map(([code, path]) => `${code} = "${path.replace(/\//g, ' > ').replace(/_/g, ' ')}"`)
            .join('\n');

        const userPrompt = `PATH MAP:\n${pathContext}\n\nINSTRUCTIONS: Create creative scene compositions`;

        const generationConfig = {
            responseMimeType: "application/json",
            responseSchema: { type: "ARRAY", items: { type: "STRING" } }
        };

        try {
            const { result, request } = await this._makeTestRequest(provider, apiKey, modelName, templatePrompt, userPrompt, generationConfig);
            const duration = Math.round(performance.now() - startTime);

            const parsed = this._parseTestResponse(provider, result);
            const isValidArray = Array.isArray(parsed) && parsed.length > 0;
            // Check if templates contain __X__ format codes
            const hasValidTemplates = isValidArray && parsed.some(t => /__[A-Z]+__/.test(String(t)));

            return {
                success: true,
                stats: {
                    responseTime: duration,
                    supportsJson: isValidArray,
                    validSchema: hasValidTemplates,
                    parsedCount: isValidArray ? parsed.length : 0,
                    parsedContent: parsed,
                    rawResponse: this._extractRawContent(result, provider),
                    request
                }
            };
        } catch (error) {
            return {
                success: false,
                error: error.message,
                stats: { responseTime: Math.round(performance.now() - startTime) }
            };
        }
    },

    /**
     * Test the Duplicate Finder AI feature.
     * Uses mock duplicate data and the configured deduplicate prompt.
     */
    async testDupeFinder(provider, apiKey, modelName) {
        const startTime = performance.now();

        const systemPrompt = Config.DEFAULT_DEDUPLICATE_PROMPT ||
            "You are an expert at organizing data. Pick the best category for duplicates.";

        // Mock duplicate data
        const mockDuplicates = [
            {
                normalized: 'dragon',
                locations: [
                    { path: 'CREATURES_and_BEINGS/Mythical_Fantasy' },
                    { path: 'CREATURES_and_BEINGS/Animals/Reptiles' }
                ]
            },
            {
                normalized: 'sunset',
                locations: [
                    { path: 'LIGHTING/Natural' },
                    { path: 'PLACES/Natural_Environments' }
                ]
            }
        ];

        const batchPrompt = mockDuplicates.map(d => {
            const pathOptions = d.locations.map(l =>
                `  - ${l.path.replace(/\//g, ' > ').replace(/_/g, ' ')}`
            ).join('\n');
            return `Wildcard: "${d.normalized}"\nFound in:\n${pathOptions}`;
        }).join('\n\n');

        const userPrompt = `Please analyze these ${mockDuplicates.length} duplicate wildcards and pick the best category for each:\n\n${batchPrompt}`;

        const generationConfig = {
            responseMimeType: "application/json",
            responseSchema: {
                type: "ARRAY",
                items: {
                    type: "OBJECT",
                    properties: {
                        wildcard: { type: "STRING" },
                        keep_path: { type: "STRING" }
                    },
                    required: ["wildcard", "keep_path"]
                }
            }
        };

        try {
            const { result, request } = await this._makeTestRequest(provider, apiKey, modelName, systemPrompt, userPrompt, generationConfig);
            const duration = Math.round(performance.now() - startTime);

            const parsed = this._parseTestResponse(provider, result);
            const isValidArray = Array.isArray(parsed) && parsed.length > 0;
            const hasCorrectShape = isValidArray && parsed[0]?.wildcard && parsed[0]?.keep_path;

            return {
                success: true,
                stats: {
                    responseTime: duration,
                    supportsJson: isValidArray,
                    validSchema: hasCorrectShape,
                    parsedCount: isValidArray ? parsed.length : 0,
                    parsedContent: parsed,
                    rawResponse: this._extractRawContent(result, provider),
                    request
                }
            };
        } catch (error) {
            return {
                success: false,
                error: error.message,
                stats: { responseTime: Math.round(performance.now() - startTime) }
            };
        }
    },

    /**
     * Run a comprehensive benchmark testing all 4 AI features.
     * @param {string} provider - API provider (gemini, openrouter, custom)
     * @param {string} apiKey - API key
     * @param {string} modelName - Model name
     * @param {function} onProgress - Callback for progress updates
     * @returns {Promise<Object>} Aggregated benchmark results
     */
    async runBenchmark(provider, apiKey, modelName, onProgress = () => { }) {
        const results = {
            generate: null,
            suggestions: null,
            templates: null,
            dupeFinder: null,
            totalTime: 0,
            passCount: 0,
            failCount: 0
        };

        const startTotal = performance.now();

        // Test 1: Generate Wildcards (existing testModel)
        onProgress({ phase: 'generate', status: 'running' });
        try {
            results.generate = await new Promise((resolve) => {
                this.testModel(provider, apiKey, modelName, resolve);
            });
            results.generate.success = results.generate.success !== false;
        } catch (e) {
            results.generate = { success: false, error: e.message };
        }
        onProgress({ phase: 'generate', status: 'complete', result: results.generate });

        // Test 2: Suggestions
        onProgress({ phase: 'suggestions', status: 'running' });
        results.suggestions = await this.testSuggestions(provider, apiKey, modelName);
        onProgress({ phase: 'suggestions', status: 'complete', result: results.suggestions });

        // Test 3: Templates
        onProgress({ phase: 'templates', status: 'running' });
        results.templates = await this.testTemplates(provider, apiKey, modelName);
        onProgress({ phase: 'templates', status: 'complete', result: results.templates });

        // Test 4: Dupe Finder
        onProgress({ phase: 'dupeFinder', status: 'running' });
        results.dupeFinder = await this.testDupeFinder(provider, apiKey, modelName);
        onProgress({ phase: 'dupeFinder', status: 'complete', result: results.dupeFinder });

        // Calculate totals
        results.totalTime = Math.round(performance.now() - startTotal);
        results.passCount = [results.generate, results.suggestions, results.templates, results.dupeFinder]
            .filter(r => r?.success).length;
        results.failCount = 4 - results.passCount;

        return results;
    },

    /**
     * Helper to make a test request with proper provider handling.
     * Similar to _prepareRequest but for testing with explicit credentials.
     */
    async _makeTestRequest(provider, apiKey, modelName, systemPrompt, userPrompt, generationConfig) {
        let url, headers, payload;

        const temp = Config.MODEL_TEMPERATURE;
        const maxTokens = Config.MODEL_MAX_TOKENS;
        const topP = Config.MODEL_TOP_P;

        if (provider === 'gemini') {
            url = `https://generativelanguage.googleapis.com/v1beta/models/${modelName}:generateContent?key=${apiKey}`;
            headers = { 'Content-Type': 'application/json' };

            const geminiGenConfig = { ...generationConfig };
            if (temp !== undefined && temp !== 0.7) geminiGenConfig.temperature = temp;
            if (maxTokens && maxTokens !== 1000) geminiGenConfig.maxOutputTokens = maxTokens;
            if (topP !== undefined && topP !== 1.0) geminiGenConfig.topP = topP;

            payload = {
                contents: [
                    { role: "user", parts: [{ text: systemPrompt }] },
                    { role: "model", parts: [{ text: "Understood." }] },
                    { role: "user", parts: [{ text: userPrompt }] }
                ],
                generationConfig: geminiGenConfig
            };
        } else {
            const isCustom = provider === 'custom';
            if (isCustom) {
                const baseUrl = document.getElementById('custom-api-url')?.value || Config?.API_URL_CUSTOM || '';
                url = baseUrl.replace(/\/$/, '') + '/chat/completions';
            } else {
                url = 'https://openrouter.ai/api/v1/chat/completions';
            }

            headers = {
                'Content-Type': 'application/json',
                ...(apiKey && { 'Authorization': `Bearer ${apiKey}` }),
                ...(provider === 'openrouter' && { 'HTTP-Referer': window.location.origin })
            };

            payload = {
                model: modelName,
                messages: [{ role: 'user', content: `${systemPrompt}\n\n${userPrompt}` }]
            };

            // Use proper response format for each provider
            if (isCustom) {
                // LMStudio and similar require json_schema format
                payload.response_format = {
                    type: 'json_schema',
                    json_schema: this._constructJsonSchema(generationConfig)
                };
            } else {
                // OpenRouter supports json_object
                payload.response_format = { type: 'json_object' };
            }

            if (temp !== undefined && temp !== 0.7) payload.temperature = temp;
            if (maxTokens && maxTokens !== 1000) payload.max_tokens = maxTokens;
            if (topP !== undefined && topP !== 1.0) payload.top_p = topP;
        }

        const response = await fetch(url, {
            method: 'POST',
            headers,
            body: JSON.stringify(payload)
        });

        if (!response.ok) {
            const text = await response.text();
            throw new Error(`HTTP ${response.status}: ${text}`);
        }

        return { result: await response.json(), request: { url, payload } };
    },

    /**
     * Helper to parse test response based on provider format.
     */
    _parseTestResponse(provider, result) {
        try {
            let contentStr;
            if (provider === 'gemini') {
                contentStr = result.candidates?.[0]?.content?.parts?.[0]?.text || '';
            } else {
                contentStr = result.choices?.[0]?.message?.content || '';
            }

            contentStr = contentStr.trim();
            const match = /```(?:json)?\s*([\s\S]*?)\s*```/.exec(contentStr);
            if (match) contentStr = match[1];

            const parsed = JSON.parse(contentStr);
            // Handle wrapped responses
            if (parsed.items && Array.isArray(parsed.items)) return parsed.items;
            return Array.isArray(parsed) ? parsed : [];
        } catch (e) {
            return [];
        }
    },

    /**
     * Helper to extract raw content from response.
     */
    _extractRawContent(result, provider) {
        if (provider === 'gemini') {
            return result.candidates?.[0]?.content?.parts?.[0]?.text || '';
        }
        return result.choices?.[0]?.message?.content || '';
    }
};
